{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e763473",
   "metadata": {},
   "source": [
    "## Notes \n",
    "- Pour l'instant, on test sur le validation set. Il faut regarder si le validation set est utiliser durant le training ou s'il est juste la pour l'évaluation. \n",
    "- Pour la traduction, le fait de traduire va peut-être faire que la variable `answer_start` va être déplacée mais je pense que ça n'a pas d'impact car on ne l'utilise pas à la validation\n",
    "- Le model anglais est train sur 4x plus de samples : est ce qu'on veut train sur plus d'epoch sur le francais \n",
    "- La traduction de la réponse est pas fait en même temps que celle de la question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a68d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = {}\n",
    "dataset_name['fr'] = 'fquad'\n",
    "dataset_name['en'] = 'squad'\n",
    "data_path = '/data/desponds/data/Question_answering/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c358d153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import preprocessing_question_answering\n",
    "datasets, tokenized = preprocessing_question_answering(dataset_name, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d953e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from training import get_trainers_question_answering\n",
    "trainers = get_trainers_question_answering(data_path, tokenized, langs = ['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff92c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from training import get_models_question_answering\n",
    "models = get_models_question_answering(trainers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e949f",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02654902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_fr_en_qa(example):\n",
    "    example['context'] = translate_fr_en(example['context'])\n",
    "    example['question'] = translate_fr_en(example['question'])\n",
    "    example['answers']['text'] = [translate_fr_en(example['answers']['text'][0])]\n",
    "    # If we can find the traduction in the text directly we update the ànswer_start` var\n",
    "    idx = example['context'].find(example['answers']['text'][0])\n",
    "    example['answers']['answer_start'] = [idx] if idx != -1 else example['answers']['answer_start']\n",
    "    return example\n",
    "\n",
    "# Translate the test split of the french dataset\n",
    "translated_fr_en = dataset_fr['valid'].map(translate_fr_en_qa, batched=True, batch_size = 32)\n",
    "\n",
    "with open('/data/desponds/data/Question_answering/translated_dataset.pickle', 'wb') as handle:\n",
    "    pickle.dump(translated_fr_en, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3475b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/data/desponds/data/Question_answering/translated_dataset.pickle', 'rb') as handle:\n",
    "    translated_fr_en = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df6542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recompute the tokens of the translated version\n",
    "from preprocessing import preprocess_validation_examples_QA\n",
    "tokenized_translated_fr_en = translated_fr_en.map(lambda examples : preprocess_validation_examples_QA(examples, 'en'), \n",
    "                                                batched=True, remove_columns=datasets['en'][\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329efbd5",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a07393",
   "metadata": {},
   "source": [
    "### Dataset FR on Camembert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ad998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ON hugging face they manage to get {\"f1\": 88.3, \"exact_match\": 78.0}\n",
    "predictions_fr, _, _ = trainers['fr'].predict(tokenized_fr['validation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066dff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits_fr, end_logits_fr = predictions_fr\n",
    "metric_fr, predicted_answers_fr, theoretical_answers_fr = compute_metrics_QA(start_logits_fr, end_logits_fr, tokenized_fr['validation'], dataset_fr[\"valid\"])\n",
    "metric_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d4ef4",
   "metadata": {},
   "source": [
    "### Dataset EN on RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import compute_metrics_QA\n",
    "predictions_en, _, _ = models['en'].predict(tokenized['en']['validation'])\n",
    "start_logits_en, end_logits_en = predictions_en\n",
    "metric_en, predicted_answers_en, theoretical_answers_en = compute_metrics_QA(start_logits_en, end_logits_en, tokenized['en']['validation'], datasets['en'][\"validation\"])\n",
    "metric_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b46b9",
   "metadata": {},
   "source": [
    "### Dataset FR on RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_fr_en, _, _ = models['en'].predict(tokenized_translated_fr_en)\n",
    "start_logits_fr_en, end_logits_fr_en = predictions_fr_en\n",
    "metric_fr_en, predicted_answers_fr_en, theoretical_answers_fr_en = compute_metrics_QA(start_logits_fr_en, end_logits_fr_en, \n",
    "                tokenized_translated_fr_en, translated_fr_en,\n",
    "               need_translation = True, base_answers = datasets['fr'][\"validation\"]\n",
    "               )\n",
    "metric_fr_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98400811",
   "metadata": {},
   "source": [
    "## Analysing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb024e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_qa = pd.DataFrame() \n",
    "results_qa['theoretical_answers'] = [th['answers']['text'][0] for th in theoretical_answers_fr]\n",
    "results_qa['predicted_answers_fr'] = [th['prediction_text'] for th in predicted_answers_fr]\n",
    "results_qa['predicted_answers_fr_en'] = [th['prediction_text'] for th in predicted_answers_fr_en]\n",
    "results_qa['predicted_answers_fr_logit'] = [th['prediction_logit'] for th in predicted_answers_fr]\n",
    "results_qa['predicted_answers_fr_en_logit'] = [th['prediction_logit'] for th in predicted_answers_fr_en]\n",
    "results_qa['len_context'] = [len(th['context'].split()) for th in dataset_fr[\"valid\"]]\n",
    "results_qa['question'] = [th['question'] for th in dataset_fr[\"valid\"]]\n",
    "results_qa['exact_match_fr'] = results_qa.apply(lambda ex : 1 if ex['theoretical_answers_fr'] == ex['predicted_answers_fr'] else 0, axis =1)\n",
    "results_qa['exact_match_fr_en'] = results_qa.apply(lambda ex : 1 if ex['theoretical_answers_fr'] == ex['predicted_answers_fr_en'] else 0, axis =1)\n",
    "results_qa['exact_match_fr_en_no_accent_lower'] = results_qa.apply(lambda ex : 1 if strip_accents_and_lower(ex['theoretical_answers_fr']) == strip_accents_and_lower(ex['predicted_answers_fr_en']) else 0, axis =1)\n",
    "results_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa79cd",
   "metadata": {},
   "source": [
    "## Use Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957aeb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/data/desponds/data/Question_answering/comparing_fr_answers.pickle', 'rb') as handle:\n",
    "    results_qa = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df24910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from helper import strip_accents_and_lower\n",
    "lev_dist = pd.DataFrame()\n",
    "lev_dist['lev_dist_fr_ratio'] = results_qa.apply(lambda ex :  \n",
    "                Levenshtein.ratio(ex['theoretical_answers_fr'],ex['predicted_answers_fr']), axis =1)\n",
    "lev_dist['lev_dist_fr_en_ratio'] = results_qa.apply(lambda ex : \n",
    "                Levenshtein.ratio(ex['theoretical_answers_fr'],ex['predicted_answers_fr_en']), axis =1)\n",
    "lev_dist['lev_dist_fr'] = results_qa.apply(lambda ex :  \n",
    "                Levenshtein.distance(ex['theoretical_answers_fr'],ex['predicted_answers_fr']), axis =1)\n",
    "lev_dist['lev_dist_fr_en'] = results_qa.apply(lambda ex : \n",
    "                Levenshtein.distance(ex['theoretical_answers_fr'],ex['predicted_answers_fr_en']), axis =1)\n",
    "lev_dist['lev_dist_fr_en_no_accent_lower'] = results_qa.apply(lambda ex : \n",
    "                Levenshtein.distance(strip_accents_and_lower(ex['theoretical_answers_fr']),strip_accents_and_lower(ex['predicted_answers_fr_en']) ), axis =1)\n",
    "\n",
    "lev_dist['lev_dist_fr_en_ratio_no_accent_lower'] = results_qa.apply(lambda ex : \n",
    "                Levenshtein.ratio(strip_accents_and_lower(ex['theoretical_answers_fr']),strip_accents_and_lower(ex['predicted_answers_fr_en']) ), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ef191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lev_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b368d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(1,1, figsize = (9,5), sharex=True, sharey=True)\n",
    "lev_dist[(lev_dist['lev_dist_fr_en'] !=0)&(lev_dist['lev_dist_fr_en'] <100) ]['lev_dist_fr_en'].hist(alpha = 0.6, bins = 30, ax = axs, label = \"No postprocessing\")\n",
    "lev_dist[(lev_dist['lev_dist_fr_en_no_accent_lower'] !=0)&(lev_dist['lev_dist_fr_en'] <100) ]['lev_dist_fr_en_no_accent_lower'].hist(alpha = 0.6, bins = 30, ax = axs, label = \"Postprrocessing : no accent, lower\")\n",
    "fig.suptitle('Levenshtein distance (caped to 100) between theoretical and predicted')\n",
    "fig.supxlabel('Levenshtein distance')\n",
    "# fig.supylabel('Count')\n",
    "axs.legend(loc = 'upper right')\n",
    "axs.set_title('Impact of postprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e75f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_exact_match(value_fr, value_fr_en):\n",
    "    print(len(results_qa[(results_qa['exact_match_fr'] == value_fr) & (results_qa['exact_match_fr_en'] == value_fr_en)]))\n",
    "    return results_qa[(results_qa['exact_match_fr'] == value_fr) & (results_qa['exact_match_fr_en'] == value_fr_en)]\n",
    "diff = diff_exact_match(value_fr = 0, value_fr_en = 1)\n",
    "diff.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58adb2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(1,1, figsize = (9,5), sharex=True, sharey=True)\n",
    "lev_dist[lev_dist['lev_dist_fr_en_ratio'] !=1]['lev_dist_fr_en_ratio'].hist(alpha = 0.4, bins = 30, ax = axs, label = \"No postprocessing\")\n",
    "lev_dist[lev_dist['lev_dist_fr_en_ratio_no_accent_lower'] !=1]['lev_dist_fr_en_ratio_no_accent_lower'].hist(alpha = 0.4, bins = 30, ax = axs, label = \"Postprocessing : no accent, lower\")\n",
    "fig.suptitle('Levenshtein ratio between theoretical and predicted')\n",
    "fig.supxlabel('Levenshtein ratio')\n",
    "axs.legend(loc = 'upper left')\n",
    "axs.set_title('Impact of postprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e084ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('/data/desponds/data/Question_answering/comparing_fr_answers.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_qa, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8d86d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d70be",
   "metadata": {},
   "source": [
    "### Metrics accepting a certain Levenstein distance with english dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfcdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_en_lev_10, predicted_answers_en_lev_10, theoretical_answers_en_lev_10 = compute_metrics_QA(\n",
    "    start_logits_en, \n",
    "    end_logits_en, \n",
    "    tokenized['en']['validation'], \n",
    "    datasets['en'][\"validation\"], \n",
    "    accept_levenstein = 10)\n",
    "metric_en_lev_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c895a71",
   "metadata": {},
   "source": [
    "### Metrics accepting a certain Levenstein distance with french translated dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fba68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_fr_en_lev_1, predicted_answers_fr_en_lev_1, theoretical_answers_fr_en_lev_1 = compute_metrics_QA(\n",
    "    start_logits_fr_en, end_logits_fr_en, \n",
    "    tokenized_translated_fr_en, translated_fr_en,\n",
    "    need_translation = True, base_answers = datasets['fr'][\"validation\"], \n",
    "    accept_levenstein = 1)\n",
    "metric_fr_en_lev_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_fr_en_lev_3, predicted_answers_fr_en_lev_3, theoretical_answers_fr_en_lev_3 = compute_metrics_QA(\n",
    "    start_logits_fr_en, \n",
    "    end_logits_fr_en, \n",
    "    tokenized_translated_fr_en, \n",
    "    translated_fr_en,\n",
    "    need_translation = True, \n",
    "    base_answers = datasets['fr'][\"validation\"], \n",
    "    accept_levenstein = 3\n",
    "               )\n",
    "metric_fr_en_lev_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa881d5",
   "metadata": {},
   "source": [
    "# Use Paraphrasing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2499eb8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the paraphrase models\n",
    "from training import get_trainers_paraphrasing, get_models_paraphrasing\n",
    "from preprocessing import preprocessing_paraphrasing\n",
    "datasets = {}\n",
    "datasets['fr'] = preprocessing_paraphrasing('fr')\n",
    "trainers = get_trainers_paraphrasing('/data/desponds/data/Paraphrase/', datasets, langs = ['fr'])\n",
    "models = get_models_paraphrasing(trainers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffbe748",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import preprocessing_paraphrasing, tokenize_paraphrasing\n",
    "from datasets import Dataset\n",
    "\n",
    "# Get the data that we want to us on the paraphrase models \n",
    "def get_df_paraphrasing_qa(lang, with_questions) :\n",
    "    para = pd.DataFrame()\n",
    "    if not with_questions : \n",
    "        para['sentence1'] = results_qa[f'theoretical_answers_fr']\n",
    "        para['sentence2'] = results_qa[f'predicted_answers_{lang}']\n",
    "    else :\n",
    "        para['sentence1'] = results_qa['question'] +\" \"+ results_qa[f'theoretical_answers_fr']\n",
    "        para['sentence2'] = results_qa['question'] +\" \"+ results_qa[f'predicted_answers_{lang}']\n",
    "    return Dataset.from_pandas(para)\n",
    "\n",
    "# Get the data on the predicted answer using Camembert \n",
    "para_fr = get_df_paraphrasing_qa('fr', with_questions = False)\n",
    "para_fr_q = get_df_paraphrasing_qa('fr', with_questions = True)\n",
    "# Get the data on the predicted answer using Roberta and translation \n",
    "para_fr_en = get_df_paraphrasing_qa('fr_en', with_questions = False)\n",
    "para_fr_en_q = get_df_paraphrasing_qa('fr_en', with_questions = True)\n",
    "\n",
    "# Tokenize both of them \n",
    "tokenizer = AutoTokenizer.from_pretrained('camembert-base')\n",
    "tokenized_fr = para_fr.map(lambda example : tokenize_paraphrasing(example, 'fr', tokenizer, with_label = False, MAX_LENGTH = 80 , truncation = 'longest_first'))\n",
    "tokenized_fr_en = para_fr_en.map(lambda example : tokenize_paraphrasing(example, 'fr', tokenizer, with_label = False, MAX_LENGTH = 80, truncation = 'longest_first' ))\n",
    "tokenized_fr_q = para_fr_q.map(lambda example : tokenize_paraphrasing(example, 'fr', tokenizer, with_label = False, MAX_LENGTH = 160 , truncation = 'longest_first'))\n",
    "tokenized_fr_en_q = para_fr_en_q.map(lambda example : tokenize_paraphrasing(example, 'fr', tokenizer, with_label = False, MAX_LENGTH = 160, truncation = 'longest_first' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a4bd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the model to get the logits\n",
    "predictions_fr = models['fr'].predict(tokenized_fr)\n",
    "predictions_fr_en = models['fr'].predict(tokenized_fr_en)\n",
    "predictions_fr_q = models['fr'].predict(tokenized_fr_q)\n",
    "predictions_fr_en_q = models['fr'].predict(tokenized_fr_en_q)\n",
    "\n",
    "#Use the logits to get the labels \n",
    "labels_fr = predictions_fr.predictions.argmax(axis =1)\n",
    "labels_fr_en = predictions_fr_en.predictions.argmax(axis =1)\n",
    "labels_fr_q = predictions_fr_q.predictions.argmax(axis =1)\n",
    "labels_fr_en_q = predictions_fr_en_q.predictions.argmax(axis =1)\n",
    "\n",
    "# Add our labels to the results df\n",
    "results_qa['paraphrase_fr'] = labels_fr\n",
    "results_qa['paraphrase_fr_en'] = labels_fr_en\n",
    "results_qa['paraphrase_fr_q'] = labels_fr_q\n",
    "results_qa['paraphrase_fr_en_q'] = labels_fr_en_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_qa[['question', 'theoretical_answers_fr', 'predicted_answers_fr', 'predicted_answers_fr_en', 'paraphrase_fr', 'paraphrase_fr_en']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91518d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_qa[['paraphrase_fr', 'paraphrase_fr_en', 'paraphrase_fr_q', 'paraphrase_fr_en_q']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442e850",
   "metadata": {},
   "source": [
    "## Using BERTscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad3266b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"fr\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fr = bertscore.compute(predictions=results_qa['predicted_answers_fr'], \n",
    "                            references=results_qa['theoretical_answers_fr'], lang=\"fr\")\n",
    "results_fr_en = bertscore.compute(predictions=results_qa['predicted_answers_fr_en'], \n",
    "                            references=results_qa['theoretical_answers_fr'], lang=\"fr\")\n",
    "results_fr_wb = bertscore.compute(predictions=results_qa['predicted_answers_fr'], \n",
    "                            references=results_qa['theoretical_answers_fr'], lang=\"fr\", rescale_with_baseline = True)\n",
    "results_fr_en_wb = bertscore.compute(predictions=results_qa['predicted_answers_fr_en'], \n",
    "                            references=results_qa['theoretical_answers_fr'], lang=\"fr\", rescale_with_baseline = True)\n",
    "results_qa['BERTscore_f1_fr'] = results_fr['f1']\n",
    "results_qa['BERTscore_f1_fr_en'] = results_fr_en['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cac386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(results_fr['f1']), np.mean(results_fr_en['f1']),np.mean(results_fr_wb['f1']), np.mean(results_fr_en_wb['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_en = [p['prediction_text'] for p in predicted_answers_en]\n",
    "ta_en = [p['answers']['text'][0] for p in theoretical_answers_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b20181",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_en = bertscore.compute(predictions=pa_en, \n",
    "                            references=ta_en, lang=\"en\")\n",
    "results_en_wb = bertscore.compute(predictions=pa_en, \n",
    "                            references=ta_en, lang=\"en\", rescale_with_baseline = True)\n",
    "np.mean(results_en['f1']), np.mean(results_en_wb['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cef976",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_qa[['question', 'theoretical_answers_fr', 'predicted_answers_fr', 'predicted_answers_fr_en', 'BERTscore_f1_fr', 'BERTscore_f1_fr_en']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34900d5a",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {\n",
    "    'task' : ['Question_answering', 'Question_answering', 'Question_answering'],\n",
    "    'model'       : ['CamemBERT', 'Roberta', 'Roberta'],\n",
    "    'train_dataset' : ['fquad', 'squad', 'squad'],\n",
    "    'nb_sample_train' : [20731,87599,87599],\n",
    "    'test_dataset' : ['fquad', 'squad', 'fquad_translated'],\n",
    "    'translated' : ['no', 'no', 'yes'],\n",
    "    'f1_score'    : [73.368852, 92.134176, 56.55880],\n",
    "    'exact_match' : [45.388958, 85.761589, 29.76787],\n",
    "    'BERTscore'   : [0.8984004, 0.964069, 0.8525224]\n",
    "    #em : 41.060225846925974 f1 :65.32442339320636 sans retraduire vers le francais\n",
    "}\n",
    "results = pd.DataFrame(data)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230b24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
