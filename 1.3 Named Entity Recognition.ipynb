{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69676f16",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06452c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = {}\n",
    "model_name['fr'] = 'camembert-base'\n",
    "model_name['en'] = 'roberta-base'\n",
    "model_translation = {}\n",
    "model_translation['fr_en'] = 'Helsinki-NLP/opus-mt-fr-en'\n",
    "model_translation['en_fr'] = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "dataset_name = 'wikiann'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2daa0",
   "metadata": {},
   "source": [
    "# Get tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en = load_dataset(dataset_name, 'en')\n",
    "dataset_fr = load_dataset(dataset_name, 'fr')\n",
    "\n",
    "tokenizer = {}\n",
    "tokenizer['fr'] = AutoTokenizer.from_pretrained(model_name['fr'])\n",
    "tokenizer['en'] = AutoTokenizer.from_pretrained(model_name['en'], add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e900557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the values for input_ids, token_type_ids, attention_mask\n",
    "def tokenize_adjust_labels(examples, tokenizer):\n",
    "    tokenized_samples = tokenizer.batch_encode_plus(examples[\"tokens\"], is_split_into_words=True, return_overflowing_tokens=True)\n",
    "\n",
    "    sample_map = tokenized_samples.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        tokenized_samples[key] = [values[i] for i in sample_map]\n",
    "    #tokenized_samples is not a datasets object so this alone won't work with Trainer API, hence map is used \n",
    "    #so the new keys [input_ids, labels (after adjustment)]\n",
    "    #can be added to the datasets dict for each train test validation split\n",
    "    total_adjusted_labels = []\n",
    "    print(len(tokenized_samples[\"input_ids\"]))\n",
    "    for k in range(0, len(tokenized_samples[\"input_ids\"])):\n",
    "        prev_wid = -1\n",
    "        word_ids_list = tokenized_samples.word_ids(batch_index=k)\n",
    "        existing_label_ids = examples[\"ner_tags\"][k]\n",
    "        i = -1\n",
    "        adjusted_label_ids = []\n",
    "     \n",
    "    for wid in word_ids_list:\n",
    "        if(wid is None):\n",
    "            adjusted_label_ids.append(-100)\n",
    "        elif(wid!=prev_wid):\n",
    "            i = i + 1\n",
    "            adjusted_label_ids.append(existing_label_ids[i])\n",
    "            prev_wid = wid\n",
    "        else:\n",
    "            label_name = label_names[existing_label_ids[i]]\n",
    "            adjusted_label_ids.append(existing_label_ids[i])\n",
    "            \n",
    "        total_adjusted_labels.append(adjusted_label_ids)\n",
    "    tokenized_samples[\"labels\"] = total_adjusted_labels\n",
    "    return tokenized_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954cb881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, language, label_all_tokens = True):\n",
    "    tokenized_inputs = tokenizer[language](examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ed7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_fr = dataset_fr.map(lambda examples : tokenize_and_align_labels(examples, 'fr'),batched=True)\n",
    "tokenized_fr = tokenized_fr.remove_columns(dataset_fr[\"train\"].column_names)\n",
    "\n",
    "tokenized_en = dataset_en.map(lambda examples : tokenize_and_align_labels(examples, 'en'),batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a18379",
   "metadata": {},
   "source": [
    "# Compute trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbb329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "        if(k not in flattened_results.keys()):\n",
    "            flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fd840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = {}\n",
    "data_collator['fr'] = DataCollatorForTokenClassification(tokenizer['fr'])\n",
    "data_collator['en'] = DataCollatorForTokenClassification(tokenizer['en'])\n",
    "\n",
    "model = {}\n",
    "model['fr'] = AutoModelForTokenClassification.from_pretrained(model_name['fr'])\n",
    "model['en'] = AutoModelForTokenClassification.from_pretrained(model_name['en'])\n",
    "\n",
    "training_args = {}\n",
    "training_args['fr'] = TrainingArguments(\n",
    "    output_dir=\"/data/desponds/data/NER/trainer_fr/\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "\n",
    "trainer = {}\n",
    "trainer['fr'] = Trainer(\n",
    "    model=model['fr'],\n",
    "    args=training_args['fr'],\n",
    "    train_dataset=tokenized_fr[\"train\"].select(range(100)),\n",
    "    eval_dataset=tokenized_fr[\"validation\"],\n",
    "    data_collator=data_collator['fr'],\n",
    "    tokenizer=tokenizer['fr'],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cd79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer['fr'].train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b5f21",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2389c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import translate_fr_en\n",
    "def translate_fr_en_qa(example):\n",
    "    example['tokens'] = translate_fr_en(' '.join(example['tokens'])).split(' ')\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ef452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Translate the test split of the french dataset\n",
    "translated_fr_en = dataset_fr['valid'].map(translate_fr_en_qa)\n",
    "\n",
    "with open('/data/desponds/data/NER/translated_dataset.pickle', 'wb') as handle:\n",
    "    pickle.dump(translated_fr_en, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72c27e",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer['fr'].predict(tokenized_fr['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e2d19",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e963e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {\n",
    "    'task' : ['NER', 'NER', 'NER'],\n",
    "    'dataset' : ['', '', ''],\n",
    "    'translated' : ['no', 'no', 'yes'],\n",
    "    'model'   : ['CamemBERT', 'Roberta', 'Roberta'],\n",
    "    'test_loss' : [????],\n",
    "    'test_accuracy' : [????]\n",
    "}\n",
    "results = pd.DataFrame(data)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
